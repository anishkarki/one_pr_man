{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9dfab6",
   "metadata": {},
   "source": [
    "# üß† The Einstein Guide to OpenSearch Analyzers: Practical Edition\n",
    "\n",
    "Welcome to the definitive, interactive guide to Text Analysis in OpenSearch. We aren't just reading theory; we are going to run it against your local OpenSearch instance.\n",
    "\n",
    "**Topics Covered:**\n",
    "1.  **Text Analysis**: The big picture.\n",
    "2.  **Analyzers**: The pre-packaged deals.\n",
    "3.  **Tokenizers**: The chopping block.\n",
    "4.  **Token Filters**: The polishers.\n",
    "5.  **Character Filters**: The sanitizers.\n",
    "6.  **Normalizers**: For keywords.\n",
    "7.  **Stemming**: Finding the root.\n",
    "8.  **Token Graphs**: Handling multi-word synonyms.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Setup\n",
    "First, let's set up a helper function to talk to your local OpenSearch (`localhost:19200`).\n",
    "We'll use this to send requests to the `_analyze` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063611c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup Complete. Helper function `analyze()` is ready.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "OPENSEARCH_URL = \"http://localhost:19200\"\n",
    "AUTH = ('admin', 'OpenSearch@2024') # Default demo credentials\n",
    "VERIFY_SSL = False\n",
    "\n",
    "def analyze(text, analyzer=None, tokenizer=None, filters=None, char_filters=None, explain=False):\n",
    "    \"\"\"\n",
    "    Helper function to call the _analyze API.\n",
    "    \"\"\"\n",
    "    url = f\"{OPENSEARCH_URL}/_analyze\"\n",
    "    \n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"explain\": explain\n",
    "    }\n",
    "    \n",
    "    if analyzer:\n",
    "        payload[\"analyzer\"] = analyzer\n",
    "    if tokenizer:\n",
    "        payload[\"tokenizer\"] = tokenizer\n",
    "    if filters:\n",
    "        payload[\"filter\"] = filters\n",
    "    if char_filters:\n",
    "        payload[\"char_filter\"] = char_filters\n",
    "        \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, auth=AUTH, verify=VERIFY_SSL)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Pretty print the tokens\n",
    "        result = response.json()\n",
    "        if explain:\n",
    "            print(json.dumps(result, indent=2))\n",
    "        else:\n",
    "            print(f\"Input: '{text}'\")\n",
    "            print(\"-\" * 40)\n",
    "            if \"tokens\" in result:\n",
    "                for token in result[\"tokens\"]:\n",
    "                    print(f\"[{token['position']}] Token: {token['token']:<15} | Type: {token['type']}\")\n",
    "            else:\n",
    "                print(\"No tokens produced.\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if 'response' in locals():\n",
    "            print(response.text)\n",
    "\n",
    "print(\"‚úÖ Setup Complete. Helper function `analyze()` is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b2837",
   "metadata": {},
   "source": [
    "## 1. Text Analysis & Analyzers\n",
    "\n",
    "**Text Analysis** is the process of breaking down text into terms.\n",
    "An **Analyzer** is the package that does this. It's composed of:\n",
    "1.  **Char Filters** (0 or more)\n",
    "2.  **Tokenizer** (Exactly 1)\n",
    "3.  **Token Filters** (0 or more)\n",
    "\n",
    "Let's look at the `standard` analyzer, which is the default. It splits on word boundaries and removes punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06c94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The 2 QUICK Brown-Foxes jumped over,e=22012,LOG: the lazy dog's back!'\n",
      "----------------------------------------\n",
      "[0] Token: the             | Type: <ALPHANUM>\n",
      "[1] Token: 2               | Type: <NUM>\n",
      "[2] Token: quick           | Type: <ALPHANUM>\n",
      "[3] Token: brown           | Type: <ALPHANUM>\n",
      "[4] Token: foxes           | Type: <ALPHANUM>\n",
      "[5] Token: jumped          | Type: <ALPHANUM>\n",
      "[6] Token: over            | Type: <ALPHANUM>\n",
      "[7] Token: e               | Type: <ALPHANUM>\n",
      "[8] Token: 22012           | Type: <NUM>\n",
      "[9] Token: log             | Type: <ALPHANUM>\n",
      "[10] Token: the             | Type: <ALPHANUM>\n",
      "[11] Token: lazy            | Type: <ALPHANUM>\n",
      "[12] Token: dog's           | Type: <ALPHANUM>\n",
      "[13] Token: back            | Type: <ALPHANUM>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the 'standard' analyzer\n",
    "text = \"The 2 QUICK Brown-Foxes jumped over,e=22012,LOG: the lazy dog's back!\"\n",
    "analyze(text, analyzer=\"standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e74d2",
   "metadata": {},
   "source": [
    "## 2. Tokenizers (The Chopping Block)\n",
    "\n",
    "The **Tokenizer** receives a stream of characters and breaks it into tokens. You must have exactly one.\n",
    "\n",
    "*   `standard`: Splits on word boundaries, removes punctuation.\n",
    "*   `whitespace`: Splits whenever it sees a space.\n",
    "*   `keyword`: Doesn't split at all. The whole text becomes one token.\n",
    "*   `pattern`: Splits based on a Regex.\n",
    "\n",
    "Let's compare `standard` vs `whitespace` vs `keyword`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429843ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Whitespace Tokenizer ---\n",
      "Input: 'email@example.com,is my email.'\n",
      "----------------------------------------\n",
      "[0] Token: email@example.com,is | Type: word\n",
      "[1] Token: my              | Type: word\n",
      "[2] Token: email.          | Type: word\n",
      "----------------------------------------\n",
      "\n",
      "--- Standard Tokenizer ---\n",
      "Input: 'email@example.com,is my email.'\n",
      "----------------------------------------\n",
      "[0] Token: email           | Type: <ALPHANUM>\n",
      "[1] Token: example.com     | Type: <ALPHANUM>\n",
      "[2] Token: is              | Type: <ALPHANUM>\n",
      "[3] Token: my              | Type: <ALPHANUM>\n",
      "[4] Token: email           | Type: <ALPHANUM>\n",
      "----------------------------------------\n",
      "\n",
      "--- Keyword Tokenizer ---\n",
      "Input: 'email@example.com,is my email.'\n",
      "----------------------------------------\n",
      "[0] Token: email@example.com,is my email. | Type: word\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"email@example.com,is my email.\"\n",
    "\n",
    "print(\"--- Whitespace Tokenizer ---\")\n",
    "analyze(text, tokenizer=\"whitespace\")\n",
    "\n",
    "print(\"\\n--- Standard Tokenizer ---\")\n",
    "analyze(text, tokenizer=\"standard\")\n",
    "\n",
    "print(\"\\n--- Keyword Tokenizer ---\")\n",
    "analyze(text, tokenizer=\"keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bba088",
   "metadata": {},
   "source": [
    "## 3. Token Filters (The Polishers)\n",
    "\n",
    "Once tokens are created, **Token Filters** can modify, add, or remove them.\n",
    "\n",
    "*   `lowercase`: Converts to lowercase.\n",
    "*   `stop`: Removes common words (the, and, is).\n",
    "*   `unique`: Removes duplicates.\n",
    "*   `synonym`: Adds synonyms.\n",
    "\n",
    "Let's build a custom chain: `whitespace` tokenizer -> `lowercase` -> `stop` -> `unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5974d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Whitespace ---\n",
      "Input: 'The THE the quick Quick QUICK'\n",
      "----------------------------------------\n",
      "[0] Token: The             | Type: <ALPHANUM>\n",
      "[1] Token: THE             | Type: <ALPHANUM>\n",
      "[2] Token: the             | Type: <ALPHANUM>\n",
      "[3] Token: quick           | Type: <ALPHANUM>\n",
      "[4] Token: Quick           | Type: <ALPHANUM>\n",
      "[5] Token: QUICK           | Type: <ALPHANUM>\n",
      "----------------------------------------\n",
      "\n",
      "--- With Lowercase + Stop + Unique ---\n",
      "Input: 'The THE the quick Quick QUICK'\n",
      "----------------------------------------\n",
      "[3] Token: quick           | Type: <ALPHANUM>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"The THE the quick Quick QUICK\"\n",
    "\n",
    "print(\"--- Raw Whitespace ---\")\n",
    "analyze(text, tokenizer=\"standard\")\n",
    "\n",
    "print(\"\\n--- With Lowercase + Stop + Unique ---\")\n",
    "analyze(text, tokenizer=\"standard\", filters=[\"lowercase\", \"stop\", \"unique\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86033a",
   "metadata": {},
   "source": [
    "## 4. Character Filters (The Sanitizers)\n",
    "\n",
    "These run **before** the tokenizer. They work on the raw string.\n",
    "\n",
    "*   `html_strip`: Removes HTML tags.\n",
    "*   `mapping`: Replaces characters (e.g., `:)` -> `_happy_`).\n",
    "\n",
    "Let's strip some HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f363c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Without Char Filter ---\n",
      "Input: '<p>I <b>love</b> OpenSearch! &copy; 2024</p>'\n",
      "----------------------------------------\n",
      "[0] Token: p               | Type: <ALPHANUM>\n",
      "[1] Token: I               | Type: <ALPHANUM>\n",
      "[2] Token: b               | Type: <ALPHANUM>\n",
      "[3] Token: love            | Type: <ALPHANUM>\n",
      "[4] Token: b               | Type: <ALPHANUM>\n",
      "[5] Token: OpenSearch      | Type: <ALPHANUM>\n",
      "[6] Token: copy            | Type: <ALPHANUM>\n",
      "[7] Token: 2024            | Type: <NUM>\n",
      "[8] Token: p               | Type: <ALPHANUM>\n",
      "----------------------------------------\n",
      "\n",
      "--- With HTML Strip ---\n",
      "Input: '<p>I <b>love</b> OpenSearch! &copy; 2024</p>'\n",
      "----------------------------------------\n",
      "[0] Token: I               | Type: <ALPHANUM>\n",
      "[1] Token: love            | Type: <ALPHANUM>\n",
      "[2] Token: OpenSearch      | Type: <ALPHANUM>\n",
      "[3] Token: ¬©               | Type: <EMOJI>\n",
      "[4] Token: 2024            | Type: <NUM>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"<p>I <b>love</b> OpenSearch! &copy; 2024</p>\"\n",
    "\n",
    "print(\"--- Without Char Filter ---\")\n",
    "analyze(text, tokenizer=\"standard\")\n",
    "\n",
    "print(\"\\n--- With HTML Strip ---\")\n",
    "analyze(text, tokenizer=\"standard\", char_filters=[\"html_strip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fe5a1",
   "metadata": {},
   "source": [
    "## 5. Normalizers (For Keywords)\n",
    "\n",
    "Normalizers are for `keyword` fields where you want exact matching but with some leniency (like case insensitivity). They **do not** tokenize.\n",
    "\n",
    "Note: You can't test a \"normalizer\" object directly in `_analyze` easily without an index, but you can simulate it by using `keyword` tokenizer + filters.\n",
    "\n",
    "Simulating a normalizer that lowercases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1fb70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'ID-123-XYZ'\n",
      "----------------------------------------\n",
      "[0] Token: id-123-xyz      | Type: word\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"ID-123-XYZ\"\n",
    "\n",
    "# A normalizer effectively does this:\n",
    "analyze(text, tokenizer=\"keyword\", filters=[\"lowercase\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c3cb",
   "metadata": {},
   "source": [
    "## 6. Stemming (Finding the Root)\n",
    "\n",
    "Stemming reduces words to their base form.\n",
    "*   `running` -> `run`\n",
    "*   `cats` -> `cat`\n",
    "\n",
    "Common algorithms: `snowball`, `porter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abeaf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The foxes are running and jumping quickly'\n",
      "----------------------------------------\n",
      "[0] Token: the             | Type: <ALPHANUM>\n",
      "[1] Token: fox             | Type: <ALPHANUM>\n",
      "[2] Token: are             | Type: <ALPHANUM>\n",
      "[3] Token: run             | Type: <ALPHANUM>\n",
      "[4] Token: and             | Type: <ALPHANUM>\n",
      "[5] Token: jump            | Type: <ALPHANUM>\n",
      "[6] Token: quick           | Type: <ALPHANUM>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"The foxes are running and jumping quickly\"\n",
    "\n",
    "# Using the 'snowball' filter\n",
    "analyze(text, tokenizer=\"standard\", filters=[\"lowercase\", \"snowball\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99061ef1",
   "metadata": {},
   "source": [
    "## 7. Token Graphs (Multi-word Synonyms)\n",
    "\n",
    "When you have synonyms that span multiple words (e.g., \"ny\" -> \"new york\"), simple token replacement gets messy.\n",
    "The `synonym_graph` filter handles this correctly by creating a graph of tokens where \"ny\" and \"new york\" occupy the same position length.\n",
    "\n",
    "*Note: This is complex to visualize in simple JSON, but look at the `positionLength` if available.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a74e90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tokens\": [\n",
      "    {\n",
      "      \"token\": \"i\",\n",
      "      \"start_offset\": 0,\n",
      "      \"end_offset\": 1,\n",
      "      \"type\": \"<ALPHANUM>\",\n",
      "      \"position\": 0\n",
      "    },\n",
      "    {\n",
      "      \"token\": \"live\",\n",
      "      \"start_offset\": 2,\n",
      "      \"end_offset\": 6,\n",
      "      \"type\": \"<ALPHANUM>\",\n",
      "      \"position\": 1\n",
      "    },\n",
      "    {\n",
      "      \"token\": \"in\",\n",
      "      \"start_offset\": 7,\n",
      "      \"end_offset\": 9,\n",
      "      \"type\": \"<ALPHANUM>\",\n",
      "      \"position\": 2\n",
      "    },\n",
      "    {\n",
      "      \"token\": \"new\",\n",
      "      \"start_offset\": 10,\n",
      "      \"end_offset\": 12,\n",
      "      \"type\": \"SYNONYM\",\n",
      "      \"position\": 3\n",
      "    },\n",
      "    {\n",
      "      \"token\": \"ny\",\n",
      "      \"start_offset\": 10,\n",
      "      \"end_offset\": 12,\n",
      "      \"type\": \"<ALPHANUM>\",\n",
      "      \"position\": 3,\n",
      "      \"positionLength\": 2\n",
      "    },\n",
      "    {\n",
      "      \"token\": \"york\",\n",
      "      \"start_offset\": 10,\n",
      "      \"end_offset\": 12,\n",
      "      \"type\": \"SYNONYM\",\n",
      "      \"position\": 4\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# We need to define a custom analyzer in the request to use synonym_graph with custom synonyms\n",
    "text = \"I live in NY\"\n",
    "\n",
    "payload = {\n",
    "  \"tokenizer\": \"standard\",\n",
    "  \"filter\": [\n",
    "    \"lowercase\",\n",
    "    {\n",
    "      \"type\": \"synonym_graph\",\n",
    "      \"synonyms\": [\"ny, new york\"]\n",
    "    }\n",
    "  ],\n",
    "  \"text\": text\n",
    "}\n",
    "\n",
    "# We'll use requests directly here since our helper assumes simple strings for filters\n",
    "response = requests.post(f\"{OPENSEARCH_URL}/_analyze\", json=payload, auth=AUTH, verify=VERIFY_SSL)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb63534",
   "metadata": {},
   "source": [
    "## 8. Real World: Analyzing `patronidata`\n",
    "\n",
    "Let's look under the hood of your live index `patronidata`.\n",
    "We will:\n",
    "1.  Fetch the **Mapping** to see which analyzers are assigned to which fields.\n",
    "2.  Fetch the **Settings** to see if any custom analyzers are defined.\n",
    "3.  Test the analyzer on a specific field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6744d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MAPPING for patronidata ---\n",
      "{\n",
      "  \"@timestamp\": {\n",
      "    \"type\": \"date\"\n",
      "  },\n",
      "  \"_raw\": {\n",
      "    \"type\": \"text\",\n",
      "    \"fields\": {\n",
      "      \"keyword\": {\n",
      "        \"type\": \"keyword\",\n",
      "        \"ignore_above\": 256\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"cribl_breaker\": {\n",
      "    \"type\": \"text\",\n",
      "    \"fields\": {\n",
      "      \"keyword\": {\n",
      "        \"type\": \"keyword\",\n",
      "        \"ignore_above\": 256\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"host\": {\n",
      "    \"properties\": {\n",
      "      \"name\": {\n",
      "        \"type\": \"text\",\n",
      "        \"fields\": {\n",
      "          \"keyword\": {\n",
      "            \"type\": \"keyword\",\n",
      "            \"ignore_above\": 256\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"message\": {\n",
      "    \"type\": \"text\",\n",
      "    \"fields\": {\n",
      "      \"keyword\": {\n",
      "        \"type\": \"keyword\",\n",
      "        \"ignore_above\": 256\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"source\": {\n",
      "    \"type\": \"text\",\n",
      "    \"fields\": {\n",
      "      \"keyword\": {\n",
      "        \"type\": \"keyword\",\n",
      "        \"ignore_above\": 256\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "--- SETTINGS for patronidata ---\n",
      "\"No custom analysis settings found.\"\n"
     ]
    }
   ],
   "source": [
    "# 1. Get Mapping & Settings\n",
    "index_name = \"patronidata\"\n",
    "\n",
    "try:\n",
    "    # Get Mapping\n",
    "    mapping_url = f\"{OPENSEARCH_URL}/{index_name}/_mapping\"\n",
    "    mapping_resp = requests.get(mapping_url, auth=AUTH, verify=VERIFY_SSL)\n",
    "    mapping_resp.raise_for_status()\n",
    "    \n",
    "    # Get Settings\n",
    "    settings_url = f\"{OPENSEARCH_URL}/{index_name}/_settings\"\n",
    "    settings_resp = requests.get(settings_url, auth=AUTH, verify=VERIFY_SSL)\n",
    "    settings_resp.raise_for_status()\n",
    "\n",
    "    print(f\"--- MAPPING for {index_name} ---\")\n",
    "    # Just printing the properties to keep it readable\n",
    "    props = mapping_resp.json()[index_name][\"mappings\"].get(\"properties\", {})\n",
    "    print(json.dumps(props, indent=2))\n",
    "    \n",
    "    print(f\"\\n--- SETTINGS for {index_name} ---\")\n",
    "    analysis_settings = settings_resp.json()[index_name][\"settings\"][\"index\"].get(\"analysis\", \"No custom analysis settings found.\")\n",
    "    print(json.dumps(analysis_settings, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8df490ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing text using the analyzer configured for field: 'message' ---\n",
      "[0] Token: error           | Type: <ALPHANUM>\n",
      "[1] Token: connection      | Type: <ALPHANUM>\n",
      "[2] Token: failed          | Type: <ALPHANUM>\n",
      "[3] Token: to              | Type: <ALPHANUM>\n",
      "[4] Token: 192.168.1.1     | Type: <NUM>\n"
     ]
    }
   ],
   "source": [
    "# 2. Test Analyzer on a Field\n",
    "# We'll try to find a text field from the mapping above. \n",
    "# If you see a field like 'message' or 'log' in the output above, replace 'message' below.\n",
    "\n",
    "field_to_test = \"message\" # Default guess, change this based on the mapping output!\n",
    "sample_text = \"Error: Connection failed to 192.168.1.1\"\n",
    "\n",
    "print(f\"--- Analyzing text using the analyzer configured for field: '{field_to_test}' ---\")\n",
    "\n",
    "url = f\"{OPENSEARCH_URL}/{index_name}/_analyze\"\n",
    "payload = {\n",
    "  \"field\": field_to_test,\n",
    "  \"text\": sample_text\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=payload, auth=AUTH, verify=VERIFY_SSL)\n",
    "    # If the field doesn't exist, this might fail or fallback to default\n",
    "    if response.status_code == 400:\n",
    "        print(f\"Field '{field_to_test}' might not exist. Check the mapping above.\")\n",
    "        print(response.text)\n",
    "    else:\n",
    "        result = response.json()\n",
    "        for token in result.get(\"tokens\", []):\n",
    "             print(f\"[{token['position']}] Token: {token['token']:<15} | Type: {token['type']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c854bce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÄÔ∏è AUDITING INDEX: patronidata\n",
      "\n",
      "--- 1. CUSTOM DEFINITIONS (Settings) ---\n",
      "‚ùå No custom analyzers defined.\n",
      "‚ùå No custom tokenizers defined.\n",
      "‚ùå No custom filters defined.\n",
      "‚ùå No custom char_filters defined.\n",
      "‚ùå No custom normalizers defined.\n",
      "\n",
      "üëâ This index relies on BUILT-IN defaults (Standard Analyzer, etc).\n",
      "\n",
      "--- 2. FIELD ASSIGNMENTS (Mapping) ---\n",
      "üìù Field '_raw' is TEXT using analyzer: 'standard (default)'\n",
      "üìù Field 'cribl_breaker' is TEXT using analyzer: 'standard (default)'\n",
      "üìù Field 'message' is TEXT using analyzer: 'standard (default)'\n",
      "üìù Field 'source' is TEXT using analyzer: 'standard (default)'\n",
      "\n",
      "--- 3. LIVE TEST ---\n",
      "\n",
      "Testing Analyzer on field '_raw' with text: 'The Quick Foxes are Running!'\n",
      "[{'token': 'the', 'start_offset': 0, 'end_offset': 3, 'type': '<ALPHANUM>', 'position': 0}, {'token': 'quick', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}, {'token': 'foxes', 'start_offset': 10, 'end_offset': 15, 'type': '<ALPHANUM>', 'position': 2}, {'token': 'are', 'start_offset': 16, 'end_offset': 19, 'type': '<ALPHANUM>', 'position': 3}, {'token': 'running', 'start_offset': 20, 'end_offset': 27, 'type': '<ALPHANUM>', 'position': 4}]\n",
      "Resulting Tokens: ['the', 'quick', 'foxes', 'are', 'running']\n",
      "\n",
      "üîç Analysis Report:\n",
      "‚úÖ Lowercasing: YES\n",
      "‚ùå Stop Words: NO (Preserved 'the', 'are')\n",
      "‚ùå Stemming: NO ('foxes' and 'running' preserved)\n",
      "‚ùå Token Graph: NO (Linear token stream)\n"
     ]
    }
   ],
   "source": [
    "# 3. Deep Dive Audit: What's inside `patronidata`?\n",
    "# Let's programmatically check for all the concepts we learned:\n",
    "# Analyzers, Tokenizers, Filters, Char Filters, Normalizers.\n",
    "\n",
    "print(f\"üïµÔ∏è‚Äç‚ôÄÔ∏è AUDITING INDEX: {index_name}\\n\")\n",
    "\n",
    "try:\n",
    "    # Re-fetch for this cell's context\n",
    "    settings = requests.get(f\"{OPENSEARCH_URL}/{index_name}/_settings\", auth=AUTH, verify=VERIFY_SSL).json()\n",
    "    mapping = requests.get(f\"{OPENSEARCH_URL}/{index_name}/_mapping\", auth=AUTH, verify=VERIFY_SSL).json()\n",
    "    \n",
    "    index_settings = settings[index_name][\"settings\"][\"index\"]\n",
    "    analysis_config = index_settings.get(\"analysis\", {})\n",
    "    \n",
    "    # 1. Check for Custom Definitions\n",
    "    print(\"--- 1. CUSTOM DEFINITIONS (Settings) ---\")\n",
    "    components = [\"analyzer\", \"tokenizer\", \"filter\", \"char_filter\", \"normalizer\"]\n",
    "    found_custom = False\n",
    "    for component in components:\n",
    "        definitions = analysis_config.get(component, {})\n",
    "        if definitions:\n",
    "            found_custom = True\n",
    "            print(f\"‚úÖ Custom {component.title()}s found: {list(definitions.keys())}\")\n",
    "            print(json.dumps(definitions, indent=2))\n",
    "        else:\n",
    "            print(f\"‚ùå No custom {component}s defined.\")\n",
    "            \n",
    "    if not found_custom:\n",
    "        print(\"\\nüëâ This index relies on BUILT-IN defaults (Standard Analyzer, etc).\")\n",
    "\n",
    "    # 2. Check Field Assignments\n",
    "    print(\"\\n--- 2. FIELD ASSIGNMENTS (Mapping) ---\")\n",
    "    props = mapping[index_name][\"mappings\"].get(\"properties\", {})\n",
    "    \n",
    "    text_fields = []\n",
    "    keyword_fields = []\n",
    "    \n",
    "    for field, config in props.items():\n",
    "        if \"type\" in config:\n",
    "            if config[\"type\"] == \"text\":\n",
    "                analyzer = config.get(\"analyzer\", \"standard (default)\")\n",
    "                print(f\"üìù Field '{field}' is TEXT using analyzer: '{analyzer}'\")\n",
    "                text_fields.append(field)\n",
    "            elif config[\"type\"] == \"keyword\":\n",
    "                normalizer = config.get(\"normalizer\", \"None\")\n",
    "                print(f\"üîë Field '{field}' is KEYWORD using normalizer: '{normalizer}'\")\n",
    "                keyword_fields.append(field)\n",
    "                \n",
    "    # 3. Live Test on Actual Fields\n",
    "    print(\"\\n--- 3. LIVE TEST ---\")\n",
    "    \n",
    "    # Test a Text Field (Stemming, Tokenization check)\n",
    "    if text_fields:\n",
    "        target_field = text_fields[0]\n",
    "        test_text = \"The Quick Foxes are Running!\"\n",
    "        print(f\"\\nTesting Analyzer on field '{target_field}' with text: '{test_text}'\")\n",
    "        \n",
    "        # We use the index-specific analyze endpoint\n",
    "        resp = requests.post(\n",
    "            f\"{OPENSEARCH_URL}/{index_name}/_analyze\", \n",
    "            json={\"field\": target_field, \"text\": test_text}, \n",
    "            auth=AUTH, verify=VERIFY_SSL\n",
    "        ).json()\n",
    "        \n",
    "        raw_tokens = resp.get(\"tokens\", [])\n",
    "        print(raw_tokens)\n",
    "        tokens = [t[\"token\"] for t in raw_tokens]\n",
    "        print(f\"Resulting Tokens: {tokens}\")\n",
    "        \n",
    "        # --- AUTOMATED CHECKS ---\n",
    "        print(\"\\nüîç Analysis Report:\")\n",
    "        \n",
    "        # Lowercase Check\n",
    "        is_lowercased = all(t.islower() for t in tokens)\n",
    "        if is_lowercased:\n",
    "            print(\"‚úÖ Lowercasing: YES\")\n",
    "        else:\n",
    "            print(\"‚ùå Lowercasing: NO (Uppercase preserved)\")\n",
    "\n",
    "        # Stop Word Check (Expect 'the' and 'are' to be removed if stop filter is on)\n",
    "        has_stops = \"the\" in tokens or \"are\" in tokens\n",
    "        if not has_stops:\n",
    "            print(\"‚úÖ Stop Words: YES (Removed 'the', 'are')\")\n",
    "        else:\n",
    "            print(\"‚ùå Stop Words: NO (Preserved 'the', 'are')\")\n",
    "\n",
    "        # Stemming Check (Expect 'foxes'->'fox' or 'running'->'run')\n",
    "        is_stemmed = (\"fox\" in tokens and \"foxes\" not in tokens) or (\"run\" in tokens and \"running\" not in tokens)\n",
    "        if is_stemmed:\n",
    "            print(\"‚úÖ Stemming: YES ('foxes' -> 'fox' / 'running' -> 'run')\")\n",
    "        else:\n",
    "            print(\"‚ùå Stemming: NO ('foxes' and 'running' preserved)\")\n",
    "\n",
    "        # Token Graph Check\n",
    "        # Graphs are present if positionLength > 1 or if multiple tokens share the same position\n",
    "        positions = [t.get(\"position\") for t in raw_tokens]\n",
    "        position_lengths = [t.get(\"positionLength\", 1) for t in raw_tokens]\n",
    "        \n",
    "        has_overlapping_positions = len(positions) != len(set(positions))\n",
    "        has_multi_position_tokens = any(pl > 1 for pl in position_lengths)\n",
    "\n",
    "        if has_overlapping_positions or has_multi_position_tokens:\n",
    "             print(\"‚úÖ Token Graph: YES (Detected multi-position tokens or synonyms)\")\n",
    "             if has_overlapping_positions:\n",
    "                 print(\"   - Found multiple tokens at the same position.\")\n",
    "             if has_multi_position_tokens:\n",
    "                 print(\"   - Found tokens spanning multiple positions.\")\n",
    "        else:\n",
    "            print(\"‚ùå Token Graph: NO (Linear token stream)\")\n",
    "    \n",
    "    # Test a Keyword Field (Normalizer check)\n",
    "    if keyword_fields:\n",
    "        target_field = keyword_fields[0]\n",
    "        test_text = \"MixedCase-Value\"\n",
    "        print(f\"\\nTesting Normalizer on field '{target_field}' with text: '{test_text}'\")\n",
    "        \n",
    "        # For keywords, _analyze shows the single token produced\n",
    "        resp = requests.post(\n",
    "            f\"{OPENSEARCH_URL}/{index_name}/_analyze\", \n",
    "            json={\"field\": target_field, \"text\": test_text}, \n",
    "            auth=AUTH, verify=VERIFY_SSL\n",
    "        ).json()\n",
    "        \n",
    "        tokens = [t[\"token\"] for t in resp.get(\"tokens\", [])]\n",
    "        print(f\"Resulting Token: {tokens}\")\n",
    "        \n",
    "        if tokens and tokens[0] == test_text:\n",
    "            print(\"Observation: Exact match (No normalization).\")\n",
    "        else:\n",
    "            print(\"Observation: Normalized (e.g., lowercased).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during audit: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
