{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70972e57",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Search Tutorial\n",
    "This notebook demonstrates how to set up **Neural Search** (Semantic Search) in OpenSearch.\n",
    "We will use the built-in **ML Commons** plugin to host a lightweight embedding model (`all-MiniLM-L6-v2`) directly within OpenSearch.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   Running OpenSearch Cluster (v2.9+)\n",
    "*   ML Commons Plugin enabled (default in official images)\n",
    "\n",
    "**Steps:**\n",
    "1.  Configure Cluster Settings for ML.\n",
    "2.  Register a Model Group.\n",
    "3.  Register and Deploy the Embedding Model.\n",
    "4.  Create an Ingest Pipeline for Text Embedding.\n",
    "5.  Create a k-NN Index.\n",
    "6.  Ingest Data (automatically vectorized).\n",
    "7.  Perform Neural Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4017210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to http::localhost:19200...\n",
      "✅ Connected to OpenSearch\n",
      "{\n",
      "  \"name\": \"744516ba3dca\",\n",
      "  \"cluster_name\": \"docker-cluster\",\n",
      "  \"cluster_uuid\": \"VC7Klh05TXOM62fJjRlVSg\",\n",
      "  \"version\": {\n",
      "    \"distribution\": \"opensearch\",\n",
      "    \"number\": \"3.3.2\",\n",
      "    \"build_type\": \"tar\",\n",
      "    \"build_hash\": \"6564992150e26aaa62d4522a220dfff5188aeb88\",\n",
      "    \"build_date\": \"2025-10-29T22:24:07.450919802Z\",\n",
      "    \"build_snapshot\": false,\n",
      "    \"lucene_version\": \"10.3.1\",\n",
      "    \"minimum_wire_compatibility_version\": \"2.19.0\",\n",
      "    \"minimum_index_compatibility_version\": \"2.0.0\"\n",
      "  },\n",
      "  \"tagline\": \"The OpenSearch Project: https://opensearch.org/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "# Adjust these if your OpenSearch is running elsewhere\n",
    "HOST = 'localhost'\n",
    "PORT = 19200\n",
    "PROTOCOL = 'http' # or https\n",
    "AUTH = ('admin', 'OpenSearch@2024') # Default credentials\n",
    "VERIFY_SSL = False\n",
    "\n",
    "BASE_URL = f\"{PROTOCOL}::{HOST}:{PORT}\"\n",
    "\n",
    "def run_request(method, endpoint, body=None):\n",
    "    url = f\"{PROTOCOL}://{HOST}:{PORT}/{endpoint}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        if method == \"GET\":\n",
    "            resp = requests.get(url, auth=AUTH, verify=VERIFY_SSL, json=body, headers=headers)\n",
    "        elif method == \"POST\":\n",
    "            resp = requests.post(url, auth=AUTH, verify=VERIFY_SSL, json=body, headers=headers)\n",
    "        elif method == \"PUT\":\n",
    "            resp = requests.put(url, auth=AUTH, verify=VERIFY_SSL, json=body, headers=headers)\n",
    "        elif method == \"DELETE\":\n",
    "            resp = requests.delete(url, auth=AUTH, verify=VERIFY_SSL, json=body, headers=headers)\n",
    "        \n",
    "        return resp\n",
    "    except Exception as e:\n",
    "        print(f\"Connection Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check Connection\n",
    "print(f\"Connecting to {BASE_URL}...\")\n",
    "resp = run_request(\"GET\", \"\")\n",
    "if resp and resp.status_code == 200:\n",
    "    print(\"✅ Connected to OpenSearch\")\n",
    "    print(json.dumps(resp.json(), indent=2))\n",
    "else:\n",
    "    print(\"❌ Failed to connect\")\n",
    "    if resp: print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34c6ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring ML Settings...\n",
      "200\n",
      "{\"acknowledged\":true,\"persistent\":{\"plugins\":{\"ml_commons\":{\"only_run_on_ml_node\":\"false\",\"model_access_control_enabled\":\"true\",\"native_memory_threshold\":\"100\",\"trusted_url_regex\":\"^https?://.*$\"}}},\"transient\":{}}\n"
     ]
    }
   ],
   "source": [
    "# 1. Configure Cluster Settings for ML\n",
    "# These settings allow the model to run on the single node and access external URLs (to download the model).\n",
    "\n",
    "ml_settings = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.only_run_on_ml_node\": False,\n",
    "        \"plugins.ml_commons.model_access_control_enabled\": True,\n",
    "        \"plugins.ml_commons.native_memory_threshold\": 100,\n",
    "        \"plugins.ml_commons.trusted_url_regex\": \"^https?://.*$\" # Allow downloading from HuggingFace\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuring ML Settings...\")\n",
    "resp = run_request(\"PUT\", \"_cluster/settings\", ml_settings)\n",
    "print(resp.status_code)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8945082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Group ID: None\n",
      "Registering Model...\n",
      "Registration Task ID: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n",
      "Task State: None\n"
     ]
    }
   ],
   "source": [
    "# 2. Register Model Group\n",
    "mg_body = {\n",
    "    \"name\": \"nlp_model_group\",\n",
    "    \"description\": \"A model group for NLP models\"\n",
    "}\n",
    "resp = run_request(\"POST\", \"_plugins/_ml/model_groups/_register\", mg_body)\n",
    "model_group_id = resp.json().get(\"model_group_id\")\n",
    "print(f\"Model Group ID: {model_group_id}\")\n",
    "\n",
    "# 3. Register Model (all-MiniLM-L6-v2)\n",
    "# This triggers a download task.\n",
    "model_body = {\n",
    "    \"name\": \"huggingface/sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"version\": \"1.0.1\",\n",
    "    \"model_format\": \"TORCH_SCRIPT\",\n",
    "    \"model_group_id\": model_group_id\n",
    "}\n",
    "\n",
    "print(\"Registering Model...\")\n",
    "resp = run_request(\"POST\", \"_plugins/_ml/models/_register\", model_body)\n",
    "task_id = resp.json().get(\"task_id\")\n",
    "print(f\"Registration Task ID: {task_id}\")\n",
    "\n",
    "# Poll for Model ID\n",
    "model_id = None\n",
    "for i in range(20):\n",
    "    time.sleep(3)\n",
    "    status_resp = run_request(\"GET\", f\"_plugins/_ml/tasks/{task_id}\")\n",
    "    state = status_resp.json().get(\"state\")\n",
    "    print(f\"Task State: {state}\")\n",
    "    if state == \"COMPLETED\":\n",
    "        model_id = status_resp.json().get(\"model_id\")\n",
    "        print(f\"Model Registered! ID: {model_id}\")\n",
    "        break\n",
    "    elif state == \"FAILED\":\n",
    "        print(\"Registration Failed\")\n",
    "        print(status_resp.text)\n",
    "        break\n",
    "\n",
    "# 4. Deploy Model\n",
    "if model_id:\n",
    "    print(f\"Deploying Model {model_id}...\")\n",
    "    deploy_resp = run_request(\"POST\", f\"_plugins/_ml/models/{model_id}/_deploy\")\n",
    "    task_id = deploy_resp.json().get(\"task_id\")\n",
    "    print(f\"Deployment Task ID: {task_id}\")\n",
    "    \n",
    "    # Poll for Deployment\n",
    "    for i in range(20):\n",
    "        time.sleep(3)\n",
    "        status_resp = run_request(\"GET\", f\"_plugins/_ml/tasks/{task_id}\")\n",
    "        state = status_resp.json().get(\"state\")\n",
    "        print(f\"Deployment State: {state}\")\n",
    "        if state == \"COMPLETED\":\n",
    "            print(\"Model Deployed Successfully!\")\n",
    "            break\n",
    "        elif state == \"FAILED\":\n",
    "            print(\"Deployment Failed\")\n",
    "            print(status_resp.text)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff9637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping pipeline creation (No Model ID)\n"
     ]
    }
   ],
   "source": [
    "# 5. Create Ingest Pipeline\n",
    "# This pipeline intercepts documents and converts the 'text' field into embeddings using the model.\n",
    "\n",
    "pipeline_id = \"nlp-ingest-pipeline\"\n",
    "pipeline_body = {\n",
    "  \"description\": \"An NLP ingest pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "          \"text\": \"text_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "if model_id:\n",
    "    print(f\"Creating Pipeline {pipeline_id}...\")\n",
    "    resp = run_request(\"PUT\", f\"_ingest/pipeline/{pipeline_id}\", pipeline_body)\n",
    "    print(resp.status_code)\n",
    "    print(resp.text)\n",
    "else:\n",
    "    print(\"Skipping pipeline creation (No Model ID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6583f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Index my-nlp-index...\n",
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"my-nlp-index\"}\n",
      "Ingesting Documents...\n",
      "Data Ingested.\n",
      "Data Ingested.\n"
     ]
    }
   ],
   "source": [
    "# 6. Create k-NN Index\n",
    "index_name = \"my-nlp-index\"\n",
    "\n",
    "# Delete if exists\n",
    "run_request(\"DELETE\", index_name)\n",
    "\n",
    "index_body = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"default_pipeline\": pipeline_id\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": { \"type\": \"text\" },\n",
    "      \"text_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 384, # Dimension for all-MiniLM-L6-v2\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"lucene\",\n",
    "          \"parameters\": {\n",
    "            \"m\": 16,\n",
    "            \"ef_construction\": 128\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"text\": { \"type\": \"text\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "print(f\"Creating Index {index_name}...\")\n",
    "resp = run_request(\"PUT\", index_name, index_body)\n",
    "print(resp.text)\n",
    "\n",
    "# 7. Ingest Data\n",
    "docs = [\n",
    "    { \"text\": \"A west coast pale ale full of citrus notes\", \"id\": \"1\" },\n",
    "    { \"text\": \"A dark stout with coffee and chocolate flavors\", \"id\": \"2\" },\n",
    "    { \"text\": \"A refreshing lager with a crisp finish\", \"id\": \"3\" },\n",
    "    { \"text\": \"A fruity IPA with tropical vibes\", \"id\": \"4\" }\n",
    "]\n",
    "\n",
    "print(\"Ingesting Documents...\")\n",
    "for doc in docs:\n",
    "    run_request(\"POST\", f\"{index_name}/_doc\", doc)\n",
    "    \n",
    "# Refresh to make searchable\n",
    "run_request(\"POST\", f\"{index_name}/_refresh\")\n",
    "print(\"Data Ingested.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da1983e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Neural Search Results ---\n"
     ]
    }
   ],
   "source": [
    "# 8. Perform Neural Search\n",
    "# We search for \"something dark to drink\" which should match the Stout (id 2) semantically,\n",
    "# even though the words don't overlap perfectly.\n",
    "\n",
    "search_query = {\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"text_embedding\": {\n",
    "        \"query_text\": \"something dark to drink\",\n",
    "        \"model_id\": model_id,\n",
    "        \"k\": 5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": [\"text\", \"id\"]\n",
    "}\n",
    "\n",
    "print(\"--- Neural Search Results ---\")\n",
    "resp = run_request(\"GET\", f\"{index_name}/_search\", search_query)\n",
    "hits = resp.json().get(\"hits\", {}).get(\"hits\", [])\n",
    "\n",
    "for hit in hits:\n",
    "    score = hit.get(\"_score\")\n",
    "    source = hit.get(\"_source\")\n",
    "    print(f\"Score: {score:.4f} | Text: {source.get('text')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
